{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4082d5",
   "metadata": {
    "id": "7f4082d5",
    "papermill": {
     "duration": 0.025071,
     "end_time": "2025-07-27T00:33:27.230771",
     "exception": false,
     "start_time": "2025-07-27T00:33:27.205700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbaca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from datasets import Dataset,load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eba405",
   "metadata": {
    "id": "20eba405",
    "papermill": {
     "duration": 0.072068,
     "end_time": "2025-07-27T00:34:07.041607",
     "exception": false,
     "start_time": "2025-07-27T00:34:06.969539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reading the data..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c16780",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"/kaggle/input/llm-fine-tune-dataset/event_text_mapping.jsonl\"\n",
    "ds = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "\n",
    "print(\"Dataset features:\", ds.features)\n",
    "print(\"Number of examples:\", len(ds))\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i in range(min(3, len(ds))):\n",
    "    print(ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89693b6",
   "metadata": {
    "id": "b89693b6",
    "papermill": {
     "duration": 0.026933,
     "end_time": "2025-07-27T00:34:07.436256",
     "exception": false,
     "start_time": "2025-07-27T00:34:07.409323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Overview and Initial Exploration\n",
    "\n",
    "We begin by loading a JSON dataset consisting of 792 examples, each containing a natural language event description and a structured `output` dictionary. The schema reveals that each event is broken down into actionable fields such as `action`, `date`, `time`, `location`, `duration`, and optional fields like `attendees`, `recurrence`, and `notes`. The initial few samples confirm the consistency in format and provide confidence that the dataset is well-suited for training models to perform structured information extraction from free-form text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457fce3",
   "metadata": {
    "id": "6457fce3",
    "papermill": {
     "duration": 0.026415,
     "end_time": "2025-07-27T00:34:07.488833",
     "exception": false,
     "start_time": "2025-07-27T00:34:07.462418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the model..!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141f650",
   "metadata": {
    "id": "9141f650",
    "papermill": {
     "duration": 0.027132,
     "end_time": "2025-07-27T00:34:07.542246",
     "exception": false,
     "start_time": "2025-07-27T00:34:07.515114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Loading with 4-bit Quantization\n",
    "\n",
    "We load the `SmolLM-360M` model using 4-bit NF4 quantization via `BitsAndBytesConfig` for efficient memory usage and faster inference. The model architecture is based on LLaMA with 32 decoder layers and linear projections quantized to 4-bit. This enables running a moderately sized language model on limited GPU resources without significantly compromising performance, making it ideal for experimentation or fine-tuning tasks on consumer hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db67e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"HuggingFaceTB/SmolLM-360M\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=bnb_config,device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3061f",
   "metadata": {
    "id": "a5f3061f",
    "papermill": {
     "duration": 0.026187,
     "end_time": "2025-07-27T00:34:17.135808",
     "exception": false,
     "start_time": "2025-07-27T00:34:17.109621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setting up the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd02f78",
   "metadata": {
    "id": "0bd02f78",
    "papermill": {
     "duration": 0.026644,
     "end_time": "2025-07-27T00:34:18.197615",
     "exception": false,
     "start_time": "2025-07-27T00:34:18.170971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Checking the text generation capability of the model..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ea7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Hello, I'm Abhijeet. How are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=25)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb43e6",
   "metadata": {
    "id": "f2bb43e6",
    "papermill": {
     "duration": 0.026027,
     "end_time": "2025-07-27T00:34:20.970605",
     "exception": false,
     "start_time": "2025-07-27T00:34:20.944578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Checking the output for our NER task..!\n",
    "- I validated the model's ability to learn from few-shot prompting by providing a structured example followed by a new user input.\n",
    "- The prompt design helps guide the model by setting a clear pattern to follow.\n",
    "- I observed that the model follows the example structure well, but tends to repeat parts of the prompt and occasionally truncates the response.\n",
    "- These issues hint at limitations in generalization from just in-context examples.\n",
    "- Fine-tuning the model on a larger set of such input-output JSON pairs could help it learn the structure more robustly.\n",
    "- With fine-tuning, the model would better internalize entity extraction logic and reduce over-reliance on prompt templates.\n",
    "- It would also improve consistency in output formatting, casing, and handling edge cases like ambiguous durations or varying phrasing.\n",
    "- Overall, fine-tuning can make the model more reliable, reduce prompt engineering overhead, and produce cleaner, more accurate extractions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ffb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds[0]\n",
    "question = ds[1]\n",
    "prompt_template = (\n",
    "    \"See the given example to extract the entities based on given input in JSON format.\\n\\n\"\n",
    "    \"Example Input: {event_text}\\n\"\n",
    "    \"Example Output: {output}\\n\"\n",
    "    \"--------------------------\\n\"\n",
    "    \"Please extract the entities for the below user input in JSON format. And do not output anything else.\\n\"\n",
    "    \"User Input: {user_input}\\n\"\n",
    ")\n",
    "\n",
    "formatted_example = {\n",
    "    \"text\": prompt_template.format(event_text=example['event_text'], output=example['output'],user_input=question['event_text'])\n",
    "}\n",
    "\n",
    "print(\">> PROMPT FOR THE MODEL:\")\n",
    "print(\"-\"*len(\"PROMPT FOR THE MODEL:\"))\n",
    "print(formatted_example['text'])\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "input_text = formatted_example['text']\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# print(f\"Input: {input_text}\")\n",
    "print(\">> Response from SLM:\".upper())\n",
    "print(\"-\"*len(\">> Response from LM:\"))\n",
    "print(decoded_output)\n",
    "print(\"\\n\\n\")\n",
    "print(\">> Actual Output:\".upper())\n",
    "print(\"-\"*len(\">> Actual Output:\"))\n",
    "print(question['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91bc2b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T09:08:11.164425Z",
     "iopub.status.busy": "2025-07-26T09:08:11.163552Z",
     "iopub.status.idle": "2025-07-26T09:08:11.167938Z",
     "shell.execute_reply": "2025-07-26T09:08:11.167251Z",
     "shell.execute_reply.started": "2025-07-26T09:08:11.164390Z"
    },
    "id": "b91bc2b9",
    "papermill": {
     "duration": 0.027976,
     "end_time": "2025-07-27T00:34:28.490924",
     "exception": false,
     "start_time": "2025-07-27T00:34:28.462948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preparing the dataset for fine-tuning task..!\n",
    "- I structured the dataset using a consistent prompt-response format to guide the model during training.\n",
    "- Using `map(batched=True)` allowed efficient batch processing while embedding each input into a few-shot prompt with a fixed example.\n",
    "- The inclusion of a reference example in every prompt establishes a clear pattern for the model to imitate during fine-tuning.\n",
    "- This consistency in formatting helps the SLM (Small Language Model) learn how to extract entities reliably in the expected JSON format.\n",
    "- By retaining both the prompt (`text`) and the ground truth (`output`), I can directly train the model in a supervised manner.\n",
    "- This setup encourages the model to understand contextual clues and align its outputs closely with structured targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(examples):\n",
    "    \"\"\"\n",
    "    Constructs a batch of prompt strings using few-shot learning format for NER-style entity extraction.\n",
    "\n",
    "    This function formats each input text in `examples['event_text']` by embedding it into a predefined\n",
    "    prompt template. It uses a fixed example (the first instance from the `raw` dataset) as a demonstration\n",
    "    to guide the model. The function returns a dictionary containing:\n",
    "\n",
    "    - \"text\": List of formatted prompt strings for each input.\n",
    "    - \"output\": Corresponding expected outputs as strings for comparison or training.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    examples : dict\n",
    "        A dictionary with two keys:\n",
    "        - 'event_text': List of user inputs to extract entities from.\n",
    "        - 'output': List of corresponding ground truth outputs in dictionary format.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "        - 'text': List of formatted prompt strings.\n",
    "        - 'output': List of expected output strings.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = (\n",
    "    \"See the given example to extract the entities based on given input in JSON format.\\n\\n\"\n",
    "    \"Example Input: {example_event_text}\\n\"\n",
    "    \"Example Output: {example_output}\\n\"\n",
    "    \"--------------------------\\n\"\n",
    "    \"Please extract the entities for the below user input in JSON format. And do not output anything else.\\n\\n\"\n",
    "    \"Human Input: {user_input}\\n\"\n",
    "    \"AI: \"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Use the first example from the raw dataset as the fixed example for the prompt\n",
    "    example_instance={}\n",
    "    example_instance[\"event_text\"] = \"\"\"Late night study session at the caf√© on 15th, Dec 2024 at 9:00 pm for 2 hours.\"\"\"\n",
    "    example_instance['output'] = \"\"\"{'action': 'study session', 'date': '15/12/2024', 'time': '9:00 PM', 'attendees': None, 'location': 'caf√©', 'duration': '2 hours', 'recurrence': None, 'notes': None}\"\"\"\n",
    "    formatted_texts = []\n",
    "\n",
    "\n",
    "    # Iterate through the batch using the length of one of the lists (assuming all lists have the same length)\n",
    "    for i in range(len(examples['event_text'])):\n",
    "        formatted_text = prompt_template.format(\n",
    "            example_event_text=example_instance['event_text'],\n",
    "            example_output=example_instance['output'],\n",
    "            user_input=examples['event_text'][i] # Access each example in the batch correctly\n",
    "        )\n",
    "        formatted_texts.append(formatted_text)\n",
    "\n",
    "\n",
    "    return {\"text\": formatted_texts, \"output\": [str(output) for output in examples['output']]} # Access each output in the batch correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset in batches using map with batched=True\n",
    "dataset = load_dataset(\"json\", data_files=data_path)['train']\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "ds_val = dataset['test']\n",
    "ds_train = dataset['train']\n",
    "ds_train = ds_train.map(serialize,batched=True)\n",
    "\n",
    "\n",
    "print(\"Formatted dataset example:\".upper())\n",
    "print(\"-\"*len(\"Formatted dataset example:\"))\n",
    "print(ds_train[2]['text']+ds_train[2]['output'])\n",
    "print(\"\\n\\n\")\n",
    "print(\"Corresponding output:\".upper())\n",
    "print(\"-\"*len(\"Corresponding output:\"))\n",
    "print(ds_train[2]['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967bb1ab",
   "metadata": {
    "id": "967bb1ab",
    "papermill": {
     "duration": 0.026662,
     "end_time": "2025-07-27T00:34:28.835109",
     "exception": false,
     "start_time": "2025-07-27T00:34:28.808447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating a `data_loader` to generate batch of training sample for fine-tuning..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepares tokenized input for supervised fine-tuning of a language model using prompt-response format.\n",
    "\n",
    "    This function takes a dictionary containing a prompt (`example[\"text\"]`) and a target output\n",
    "    (`example[\"output\"]`), concatenates them into a single training string, and tokenizes it using\n",
    "    the provided tokenizer. It ensures that only the target portion contributes to the training loss\n",
    "    by masking the prompt tokens with -100 in the label tensor.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    example : dict\n",
    "        A dictionary with:\n",
    "        - 'text': The input prompt string used to condition the model.\n",
    "        - 'output': The expected target string to be predicted by the model.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing:\n",
    "        - 'input_ids': Token IDs of the concatenated prompt and target.\n",
    "        - 'attention_mask': Attention mask for the input sequence.\n",
    "        - 'labels': Token IDs with prompt tokens masked (-100) to compute loss only on the target.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = example[\"text\"]\n",
    "    target = example[\"output\"]\n",
    "\n",
    "\n",
    "    # Concatenate prompt and expected output as training input\n",
    "    full_text = prompt + target\n",
    "    # print(f\"Full Text:\\n{full_text}\")\n",
    "\n",
    "\n",
    "    # Tokenize the full sequence\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "\n",
    "    # Create labels: only the target portion should contribute to loss\n",
    "    labels = input_ids.clone()\n",
    "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])-1\n",
    "    labels[0,:prompt_len] = -100  # Mask out prompt tokens\n",
    "\n",
    "    # Final training batch\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids.squeeze(1),\n",
    "        \"attention_mask\": attention_mask.squeeze(1),\n",
    "        \"labels\": labels.squeeze(1),\n",
    "        \"prompt_len\":prompt_len\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tokenizer.pad(batch, padding = True,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce02a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = ds_train.map(tokenize_fn, remove_columns=ds_train.column_names)\n",
    "train_loader = DataLoader(tokenized_train, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e9a60",
   "metadata": {
    "id": "6c8e9a60",
    "papermill": {
     "duration": 0.027339,
     "end_time": "2025-07-27T00:34:30.302230",
     "exception": false,
     "start_time": "2025-07-27T00:34:30.274891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- I applied the `tokenize_fn` to the training dataset using `map`, which ensures each example is tokenized consistently.\n",
    "- Removing original columns keeps the dataset lean and avoids redundancy during training.\n",
    "- I wrapped the tokenized dataset into a `DataLoader` to enable efficient batching, shuffling, and feeding into the model.\n",
    "- The use of `collate_fn` ensures dynamic padding and batch formatting, making the training pipeline robust to variable input lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339351b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(\"BATCH KEYS:\")\n",
    "print(\"-\"*len(\"BATCH KEYS:\"))\n",
    "print(batch.keys())\n",
    "print(\"\\n\\n\")\n",
    "print(\"-\"*len(\"TRAINING BATCH EXAMPLE:  \"))\n",
    "print(\"SAMPLE BATCH EXAMPLES:\")\n",
    "print(\"-\"*len(\"TRAINING BATCH EXAMPLE:  \"))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "for i in range(batch['input_ids'].shape[0]):\n",
    "    prompt_len = batch['prompt_len'][i].item()\n",
    "    input_text = tokenizer.decode(batch['input_ids'][i][0],skip_special_tokens=True)\n",
    "    label_text = tokenizer.decode(batch['labels'][i][0][prompt_len:],skip_special_tokens=True)\n",
    "\n",
    "    print(\"-\"*len(\"EXAMPLE:    \"))\n",
    "    print(f\"EXAMPLE: {i+1}\")\n",
    "    print(\"-\"*len(\"EXAMPLE:    \"))\n",
    "    print(f\"{input_text}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"OUTPUT:\")\n",
    "    print(\"-\"*len(\"OUTPUT:\"))\n",
    "    print(f\"{label_text}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "del batch['prompt_len']\n",
    "\n",
    "# ‚úÖ Move batch tensors to same device\n",
    "batch = {k: v.to(device).squeeze(1) for k, v in batch.items()}\n",
    "\n",
    "output = model(**batch)\n",
    "loss = output.loss\n",
    "print(f\"Loss on Training Sample: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23d068",
   "metadata": {
    "id": "0f23d068",
    "papermill": {
     "duration": 0.028062,
     "end_time": "2025-07-27T00:34:30.767507",
     "exception": false,
     "start_time": "2025-07-27T00:34:30.739445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675347b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_prompt(example):\n",
    "    example_prompt = {\n",
    "        \"event_text\": [example[\"event_text\"]],\n",
    "        \"output\": [\"\"]  # we don't use actual output at test time\n",
    "    }\n",
    "    prompt_data = serialize(example_prompt)\n",
    "    return prompt_data[\"text\"][0]  # return serialized string\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_fn_val(example):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepares tokenized input for supervised fine-tuning of a language model using prompt-response format.\n",
    "\n",
    "    This function takes a dictionary containing a prompt (`example[\"text\"]`) and a target output\n",
    "    (`example[\"output\"]`), concatenates them into a single training string, and tokenizes it using\n",
    "    the provided tokenizer. It ensures that only the target portion contributes to the training loss\n",
    "    by masking the prompt tokens with -100 in the label tensor.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    example : dict\n",
    "        A dictionary with:\n",
    "        - 'text': The input prompt string used to condition the model.\n",
    "        - 'output': The expected target string to be predicted by the model.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing:\n",
    "        - 'input_ids': Token IDs of the concatenated prompt and target.\n",
    "        - 'attention_mask': Attention mask for the input sequence.\n",
    "        - 'labels': Token IDs with prompt tokens masked (-100) to compute loss only on the target.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prepare_test_prompt(example)\n",
    "    target = str(example.get(\"output\",\"\"))\n",
    "\n",
    "\n",
    "    # Concatenate prompt and expected output as training input\n",
    "    full_text = prompt + target\n",
    "    # print(f\"Full Text:\\n{full_text}\")\n",
    "\n",
    "\n",
    "    # Tokenize the full sequence\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "\n",
    "\n",
    "    # Create labels: only the target portion should contribute to loss\n",
    "    labels = input_ids.clone()\n",
    "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])-1\n",
    "    labels[0,:prompt_len] = -100  # Mask out prompt tokens\n",
    "\n",
    "    # Final training batch\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids.squeeze(1),\n",
    "        \"attention_mask\": attention_mask.squeeze(1),\n",
    "        \"labels\": labels.squeeze(1),\n",
    "        \"prompt_len\":prompt_len,\n",
    "\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val = ds_val.map(tokenize_fn_val, remove_columns=ds_val.column_names)\n",
    "val_loader = DataLoader(tokenized_val, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))\n",
    "print(\"BATCH KEYS:\")\n",
    "print(\"-\"*len(\"BATCH KEYS:\"))\n",
    "print(batch.keys())\n",
    "print(\"\\n\\n\")\n",
    "print(\"-\"*len(\"TRAINING BATCH EXAMPLE:  \"))\n",
    "print(\"SAMPLE BATCH EXAMPLES:\")\n",
    "print(\"-\"*len(\"TRAINING BATCH EXAMPLE:  \"))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "for i in range(batch['input_ids'].shape[0]):\n",
    "    prompt_len = batch['prompt_len'][i].item()\n",
    "    input_text = tokenizer.decode(batch['input_ids'][i][0],skip_special_tokens=True)\n",
    "    label_text = tokenizer.decode(batch['labels'][i][0][prompt_len:],skip_special_tokens=True)\n",
    "\n",
    "    print(\"-\"*len(\"EXAMPLE:    \"))\n",
    "    print(f\"EXAMPLE: {i+1}\")\n",
    "    print(\"-\"*len(\"EXAMPLE:    \"))\n",
    "    print(f\"{input_text}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"OUTPUT:\")\n",
    "    print(\"-\"*len(\"OUTPUT:\"))\n",
    "    print(f\"{label_text}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042bffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_on_val_set(model,data_loader,device):\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_loss=0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader,desc=\"Evaluating loss on entire set...\"):\n",
    "\n",
    "            del batch['prompt_len']\n",
    "\n",
    "            # ‚úÖ Move batch tensors to same device\n",
    "            batch = {k: v.to(device).squeeze(1) for k, v in batch.items()}\n",
    "\n",
    "            output = model(**batch)\n",
    "            batch_loss = output.loss\n",
    "\n",
    "            total_loss+=batch_loss\n",
    "\n",
    "\n",
    "    return total_loss.item() / data_loader.__len__()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085508e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_loss_on_val_set(model,train_loader,device))\n",
    "print(compute_loss_on_val_set(model,val_loader,device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3c004",
   "metadata": {
    "id": "e0e3c004",
    "papermill": {
     "duration": 0.035023,
     "end_time": "2025-07-27T00:34:57.107667",
     "exception": false,
     "start_time": "2025-07-27T00:34:57.072644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setting up PEFT config..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a14be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(peft_model, lora_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2a69a",
   "metadata": {
    "id": "07b2a69a",
    "papermill": {
     "duration": 0.036202,
     "end_time": "2025-07-27T00:34:57.349582",
     "exception": false,
     "start_time": "2025-07-27T00:34:57.313380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Getting started with Training Loop..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(cpu=False, split_batches=False)\n",
    "peft_model = accelerator.prepare(peft_model)\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "peft_model = peft_model.to(device)\n",
    "\n",
    "optimizer = AdamW(peft_model.parameters(), lr=2e-4)\n",
    "\n",
    "epochs = 10\n",
    "step_count = 0\n",
    "\n",
    "loss_metric = {}\n",
    "loss_metric['train_loss']=[]\n",
    "loss_metric['val_loss']=[]\n",
    "\n",
    "print(\"-\"*len(\"TRAINING LOOP BEGINS:  \"))\n",
    "print(\"TRAINING LOOP BEGINS:\")\n",
    "print(\"-\"*len(\"TRAINING LOOP BEGINS:  \"))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    peft_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    print(f\"\\nüîÅ Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        del batch['prompt_len']\n",
    "        batch = {k: v.to(device).squeeze(1) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "        outputs = peft_model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        step_count += 1\n",
    "\n",
    "        if step_count % 50 == 0:\n",
    "            print(f\"üîπ Step {step_count}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = compute_loss_on_val_set(peft_model,train_loader,device)\n",
    "    avg_val_loss = compute_loss_on_val_set(peft_model,val_loader,device)\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1} completed. Avg Trainig Loss: {avg_loss:.4f}. Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    loss_metric['train_loss'].append(avg_loss)\n",
    "    loss_metric['val_loss'].append(avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [i for i in range(len(loss_metric['train_loss']))]\n",
    "plt.plot(x,loss_metric['train_loss'])\n",
    "plt.plot(x,loss_metric['val_loss'])\n",
    "plt.legend(['Train','Val'])\n",
    "plt.xlabel('No. of Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b00a15",
   "metadata": {
    "id": "37b00a15",
    "papermill": {
     "duration": 0.187464,
     "end_time": "2025-07-27T01:13:43.027298",
     "exception": false,
     "start_time": "2025-07-27T01:13:42.839834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Checking performance for the NER task..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc89151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dicts(result_dict, output_dict):\n",
    "    comparison = {}\n",
    "    for key in output_dict.keys():\n",
    "        expected = output_dict[key]\n",
    "        predicted = result_dict.get(key, None)\n",
    "\n",
    "        # Normalize string values\n",
    "        if isinstance(expected, str) and isinstance(predicted, str):\n",
    "            expected = expected.strip().lower()\n",
    "            predicted = predicted.strip().lower()\n",
    "\n",
    "        # For lists: compare ignoring order\n",
    "        if isinstance(expected, list) and isinstance(predicted, list):\n",
    "            correct = sorted(expected) == sorted(predicted)\n",
    "        else:\n",
    "            correct = expected == predicted\n",
    "\n",
    "        comparison[key] = {\n",
    "            \"expected\": output_dict[key],\n",
    "            \"predicted\": result_dict.get(key, None),\n",
    "            \"match\": correct\n",
    "        }\n",
    "\n",
    "    return comparison\n",
    "\n",
    "def dict_accuracy(result_dict, output_dict):\n",
    "    comp = compare_dicts(result_dict, output_dict)\n",
    "    matches = sum(1 for k in comp if comp[k]['match'])\n",
    "    acc = {key: int(comp[key][\"match\"]) for key in comp}\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(model, tokenizer, example):\n",
    "    model.eval()\n",
    "    prompt = prepare_test_prompt(example)\n",
    "    # print(prompt)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Get generated text *excluding* the input\n",
    "    generated_ids = output_ids[:, input_length:]\n",
    "    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    try:\n",
    "        result_dict = ast.literal_eval(decoded.strip())\n",
    "    except Exception as e:\n",
    "        result_dict = {\"error\": str(e), \"raw_output\": decoded}\n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99abb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds_train[1]\n",
    "predicted_dict = run_generation(peft_model, tokenizer, example)\n",
    "\n",
    "print(\"Predicted:\", predicted_dict)\n",
    "print(\"Actual:\", example[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56c2c0",
   "metadata": {
    "id": "0d56c2c0",
    "papermill": {
     "duration": 0.243599,
     "end_time": "2025-07-27T01:13:50.918255",
     "exception": false,
     "start_time": "2025-07-27T01:13:50.674656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Saving the model and tokenizer..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50cf689",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"lora-ner-model\")\n",
    "tokenizer.save_pretrained(\"lora-ner-model\")\n",
    "\n",
    "!tar -zcvf lora-ner-model.tar.gz /kaggle/working/lora-ner-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e235f3",
   "metadata": {
    "id": "31e235f3",
    "papermill": {
     "duration": 0.18796,
     "end_time": "2025-07-27T01:13:52.832759",
     "exception": false,
     "start_time": "2025-07-27T01:13:52.644799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Calculating accuracy for NER task..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comp = ast.literal_eval(ds_train[0]['output']).keys()\n",
    "\n",
    "result_df = {}\n",
    "for i in tqdm(range(ds_train.__len__())):\n",
    "    example = ds_train[i]\n",
    "    predicted_dict = run_generation(peft_model, tokenizer, example)\n",
    "    example['output'] = ast.literal_eval(example['output'])\n",
    "    matches = dict_accuracy(predicted_dict,example['output'])\n",
    "\n",
    "    for com in comp:\n",
    "        if com in result_df.keys():\n",
    "            result_df[com].append(matches[com])\n",
    "        else:\n",
    "            result_df[com]=[]\n",
    "            result_df[com].append(matches[com])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result_df)\n",
    "print(\"Average accuracy for each `NER` on training set..!\")\n",
    "print(\"\\n\")\n",
    "print(result_df.describe().loc['mean'])\n",
    "result_df.to_csv('Train_Error.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = ast.literal_eval(ds_train[0]['output']).keys()\n",
    "\n",
    "result_df = {}\n",
    "for i in tqdm(range(ds_val.__len__())):\n",
    "    example = ds_val[i]\n",
    "    predicted_dict = run_generation(peft_model, tokenizer, example)\n",
    "    # example['output'] = ast.literal_eval(example['output'])\n",
    "    matches = dict_accuracy(predicted_dict,example['output'])\n",
    "\n",
    "    for com in comp:\n",
    "        if com in result_df.keys():\n",
    "            result_df[com].append(matches[com])\n",
    "        else:\n",
    "            result_df[com]=[]\n",
    "            result_df[com].append(matches[com])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result_df)\n",
    "print(\"Average accuracy for each `NER` on validation set..!\")\n",
    "print(\"\\n\")\n",
    "print(result_df.describe().loc['mean'])\n",
    "result_df.to_csv('Val_Error.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04124b",
   "metadata": {
    "id": "8e04124b",
    "papermill": {
     "duration": 0.223746,
     "end_time": "2025-07-27T02:34:56.328118",
     "exception": false,
     "start_time": "2025-07-27T02:34:56.104372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a08a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
