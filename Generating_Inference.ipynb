{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFTTMIuNJ-Oq"
   },
   "source": [
    "## Install & Imports..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets accelerate peft trl\n",
    "# !pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "from datasets import Dataset,load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Rx_BUs5KD_U"
   },
   "source": [
    "## Helper Functions..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_prompt(example):\n",
    "    example_prompt = {\n",
    "        \"event_text\": [example[\"event_text\"]],\n",
    "        \"output\": [\"\"]  # we don't use actual output at test time\n",
    "    }\n",
    "    prompt_data = serialize(example_prompt)\n",
    "    return prompt_data[\"text\"][0]  # return serialized string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(examples):\n",
    "    \"\"\"\n",
    "    Constructs a batch of prompt strings using few-shot learning format for NER-style entity extraction.\n",
    "\n",
    "    This function formats each input text in `examples['event_text']` by embedding it into a predefined\n",
    "    prompt template. It uses a fixed example (the first instance from the `raw` dataset) as a demonstration\n",
    "    to guide the model. The function returns a dictionary containing:\n",
    "\n",
    "    - \"text\": List of formatted prompt strings for each input.\n",
    "    - \"output\": Corresponding expected outputs as strings for comparison or training.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    examples : dict\n",
    "        A dictionary with two keys:\n",
    "        - 'event_text': List of user inputs to extract entities from.\n",
    "        - 'output': List of corresponding ground truth outputs in dictionary format.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "        - 'text': List of formatted prompt strings.\n",
    "        - 'output': List of expected output strings.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = (\n",
    "    \"See the given example to extract the entities based on given input in JSON format.\\n\\n\"\n",
    "    \"If anything can't be extracted, use None.\\n\\n\"\n",
    "    \"Example Input: {example_event_text}\\n\"\n",
    "    \"Example Output: {example_output}\\n\"\n",
    "    \"--------------------------\\n\"\n",
    "    \"Please extract the entities for the below user input in JSON format. And do not output anything else.\\n\\n\"\n",
    "    \"Human Input: {user_input}\\n\"\n",
    "    \"AI: \"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Use the first example from the raw dataset as the fixed example for the prompt\n",
    "    example_instance={}\n",
    "    example_instance[\"event_text\"] = \"\"\"Late night study session at the café on 15th, Dec 2024 at 9:00 pm for 2 hours.\"\"\"\n",
    "    example_instance['output'] = \"\"\"{'action': 'study session', 'date': '15/12/2024', 'time': '9:00 PM', 'attendees': None, 'location': 'café', 'duration': '2 hours', 'recurrence': None, 'notes': None}\"\"\"\n",
    "    formatted_texts = []\n",
    "\n",
    "\n",
    "    # Iterate through the batch using the length of one of the lists (assuming all lists have the same length)\n",
    "    for i in range(len(examples['event_text'])):\n",
    "        formatted_text = prompt_template.format(\n",
    "            example_event_text=example_instance['event_text'],\n",
    "            example_output=example_instance['output'],\n",
    "            user_input=examples['event_text'][i] # Access each example in the batch correctly\n",
    "        )\n",
    "        formatted_texts.append(formatted_text)\n",
    "\n",
    "\n",
    "    return {\"text\": formatted_texts, \"output\": [str(output) for output in examples['output']]} # Access each output in the batch correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(model, tokenizer, example):\n",
    "    model.eval()\n",
    "    prompt = prepare_test_prompt(example)\n",
    "    # print(prompt)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Get generated text *excluding* the input\n",
    "    generated_ids = output_ids[:, input_length:]\n",
    "    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    try:\n",
    "        result_dict = ast.literal_eval(decoded.strip())\n",
    "    except Exception as e:\n",
    "        result_dict = {\"error\": str(e), \"raw_output\": decoded}\n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yytZqxWdKOPO"
   },
   "source": [
    "## Setting and Loading fine-tuned model from HF..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up `device`\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading base model..\n",
    "\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM-360M\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=bnb_config,device_map={\"\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the fine-tuned `PEFT` model & tokenizer from `HFHub` and integrating the adapters with `base` model\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, \"abhxaxhbshxahxn/lora-ner-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"abhxaxhbshxahxn/lora-ner-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOwyWVAnLLwn"
   },
   "source": [
    "## Reading the data..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"/content/drive/MyDrive/Fine-Tuning Assignment/event_text_mapping.jsonl\"\n",
    "ds = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "\n",
    "print(\"Dataset features:\", ds.features)\n",
    "print(\"Number of examples:\", len(ds))\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i in range(min(3, len(ds))):\n",
    "    print(ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKYG3YafLVsQ"
   },
   "source": [
    "## Generating the inference for a given `example`..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds[786]\n",
    "predicted_dict = run_generation(peft_model, tokenizer, example)\n",
    "\n",
    "print(\"Input:\", example['event_text'])\n",
    "print(\"\\n\")\n",
    "print(\"Predicted:\", predicted_dict)\n",
    "print(\"\\n\")\n",
    "print(\"Actual:\", example[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {}\n",
    "example['event_text'] = {\"Gotcha! Let's discuss sports weekly on each Friday at 6 in the evening, over a ZOom call beginning 1st May,2024!\"}\n",
    "example['output']={}\n",
    "predicted_dict = run_generation(peft_model, tokenizer, example)\n",
    "\n",
    "print(\"Input:\", example['event_text'])\n",
    "print(\"\\n\")\n",
    "print(\"Predicted:\", predicted_dict)\n",
    "print(\"\\n\")\n",
    "print(\"Actual:\", example[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCFS9FYSNvk3"
   },
   "source": [
    "-----------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
