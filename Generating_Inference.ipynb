{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFTTMIuNJ-Oq"
   },
   "source": [
    "## Install & Imports..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k5gk9TTnwEuD"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets accelerate peft trl\n",
    "# !pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8W0vNqNBwKwY"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "from datasets import Dataset,load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Rx_BUs5KD_U"
   },
   "source": [
    "## Helper Functions..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ouLC5i2vJ4EH"
   },
   "outputs": [],
   "source": [
    "def prepare_test_prompt(example):\n",
    "    example_prompt = {\n",
    "        \"event_text\": [example[\"event_text\"]],\n",
    "        \"output\": [\"\"]  # we don't use actual output at test time\n",
    "    }\n",
    "    prompt_data = serialize(example_prompt)\n",
    "    return prompt_data[\"text\"][0]  # return serialized string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rp--M5RYJ3_Q"
   },
   "outputs": [],
   "source": [
    "def serialize(examples):\n",
    "    \"\"\"\n",
    "    Constructs a batch of prompt strings using few-shot learning format for NER-style entity extraction.\n",
    "\n",
    "    This function formats each input text in `examples['event_text']` by embedding it into a predefined\n",
    "    prompt template. It uses a fixed example (the first instance from the `raw` dataset) as a demonstration\n",
    "    to guide the model. The function returns a dictionary containing:\n",
    "\n",
    "    - \"text\": List of formatted prompt strings for each input.\n",
    "    - \"output\": Corresponding expected outputs as strings for comparison or training.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    examples : dict\n",
    "        A dictionary with two keys:\n",
    "        - 'event_text': List of user inputs to extract entities from.\n",
    "        - 'output': List of corresponding ground truth outputs in dictionary format.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "        - 'text': List of formatted prompt strings.\n",
    "        - 'output': List of expected output strings.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = (\n",
    "    \"See the given example to extract the entities based on given input in JSON format.\\n\\n\"\n",
    "    \"If anything can't be extracted, use None.\\n\\n\"\n",
    "    \"Example Input: {example_event_text}\\n\"\n",
    "    \"Example Output: {example_output}\\n\"\n",
    "    \"--------------------------\\n\"\n",
    "    \"Please extract the entities for the below user input in JSON format. And do not output anything else.\\n\\n\"\n",
    "    \"Human Input: {user_input}\\n\"\n",
    "    \"AI: \"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Use the first example from the raw dataset as the fixed example for the prompt\n",
    "    example_instance={}\n",
    "    example_instance[\"event_text\"] = \"\"\"Late night study session at the café on 15th, Dec 2024 at 9:00 pm for 2 hours.\"\"\"\n",
    "    example_instance['output'] = \"\"\"{'action': 'study session', 'date': '15/12/2024', 'time': '9:00 PM', 'attendees': None, 'location': 'café', 'duration': '2 hours', 'recurrence': None, 'notes': None}\"\"\"\n",
    "    formatted_texts = []\n",
    "\n",
    "\n",
    "    # Iterate through the batch using the length of one of the lists (assuming all lists have the same length)\n",
    "    for i in range(len(examples['event_text'])):\n",
    "        formatted_text = prompt_template.format(\n",
    "            example_event_text=example_instance['event_text'],\n",
    "            example_output=example_instance['output'],\n",
    "            user_input=examples['event_text'][i] # Access each example in the batch correctly\n",
    "        )\n",
    "        formatted_texts.append(formatted_text)\n",
    "\n",
    "\n",
    "    return {\"text\": formatted_texts, \"output\": [str(output) for output in examples['output']]} # Access each output in the batch correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mDBsiLz4KMAX"
   },
   "outputs": [],
   "source": [
    "def run_generation(model, tokenizer, example):\n",
    "    model.eval()\n",
    "    prompt = prepare_test_prompt(example)\n",
    "    # print(prompt)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Get generated text *excluding* the input\n",
    "    generated_ids = output_ids[:, input_length:]\n",
    "    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    try:\n",
    "        result_dict = ast.literal_eval(decoded.strip())\n",
    "    except Exception as e:\n",
    "        result_dict = {\"error\": str(e), \"raw_output\": decoded}\n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yytZqxWdKOPO"
   },
   "source": [
    "## Setting and Loading fine-tuned model from HF..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kf57RTTBwrBQ",
    "outputId": "df75c0b1-0567-441b-87ec-22b689df82b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting up `device`\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "c0a2091e57004fd69fa8ed823f2bf82b",
      "4a821d79a1d04b88a0aff424f7b1e4a4",
      "a1a6d930c6344a2e9f01dd88e7b9d17a",
      "bd7d022db94b42d4a59bf36f93a21852",
      "efdde695960141a48abd62fd7d8bb5c7",
      "290dfe257d2a4b619008498447c71b78",
      "f555d78613f745e0a8acd9c5f369fd64",
      "bd365715e3834791a47c5288cd0366f9",
      "8b9e4dd5a54f413383f343676e62b5ff",
      "19f86f093a754703848e27bd3f544f64",
      "c27f212ac9b2485189814e535582b98f",
      "3a1701dd422d487fa2e7e0414d9b88b7",
      "df3e9b9fe8de4827a235f2c01282b966",
      "167ff8d1d06d45a985e5910bdf8bc390",
      "25217c15126d4557ab29a5620bce5f56",
      "910541a9eccc4914b1766f978e2f2954",
      "7149837f35a64ff1926ca0ae341817fe",
      "a33e152aa5ea4403b74fecba448f1121",
      "b853b7eba86948dabfd93b7b7ec54afa",
      "825c3f8c7fb34d239a0426c0ff8c1348",
      "378c91a5296549ada54bd40c99011d16",
      "f1d4bda775c0405e86153301e96b4b22",
      "f2f269302c6c445789f7a6dcda46d23a",
      "fc9e481950184cac985dd2c83573a5ff",
      "4ae03ffaa10a455bbe917af7b63de874",
      "6271832d0cd44279bbefa5c9e45afa93",
      "8cdbd8c3dd144ec891f0e8a6188bed74",
      "12cdbfdfe8ea40a6b90f444c2dc3a06c",
      "6fecf0a5c46d45c7a02fa7edba4cd9df",
      "059111dbddd8452784414a65884fe8d9",
      "136d9c7d7cae4da489fed4a5ea3fc7c6",
      "885a61afa6c04f50b9a50113d616f64a",
      "d1bef2215c6c4082b5d5bd40f2efd11d"
     ]
    },
    "id": "j4Nt12FxxATl",
    "outputId": "c0339d33-b8c6-42a8-c99e-701d52357730"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a2091e57004fd69fa8ed823f2bf82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1701dd422d487fa2e7e0414d9b88b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f269302c6c445789f7a6dcda46d23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading base model..\n",
    "\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM-360M\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=bnb_config,device_map={\"\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "8bcf5a045b92478dbfd16a0470f89012",
      "2c31211bb3ef479caa77d3483c94e8ca",
      "74ad0da721bd473bb116ef41642dd3c5",
      "cf64bf9411e948138e8cce995dce28bd",
      "5f46a451153e4e88927c8fe0fc5065ae",
      "378b2b92603a467b85bc8b5a4ba9cb31",
      "5d72cab7695443f28c4c2680d2384adb",
      "ccb3b6c754af4c7c97f30ed2f7a1a7a9",
      "a25ebeae37e54116b28ef40e7ff35531",
      "5eba8acfb808408ab80b5a18edb90ac1",
      "4de692d7351c4abdad072eb0c7f34259",
      "fd17cea451ea43eca0b8eb1a02a3ad95",
      "fba89dce160a4f8aae8e5a206ea610f1",
      "01487643c472484ab8a0a4e758cc2f39",
      "19e27b9e9d584f28ac21b1193c118aeb",
      "5e9b06535f8842e5a2ed04f7ab6aed94",
      "14538498c3b940f496bfe648e2d40155",
      "a2e023c1580e4d4f9c78287d5d6f9f0b",
      "6a2115c2bb7346ba95da4a44c658ea7e",
      "bbd1f0793714493c8aa80f3d7562b786",
      "726d960a382342dc91a4754ca9a9cfcf",
      "f6aa47b205db4711a0cc4078e3cb7898",
      "e38eafeb773a4ba182ec6157e911d82a",
      "7c366452019c495da52fa19f2d0f074e",
      "f7587c3c20b84ab7a1c4716748f0860a",
      "dcf644a8532148748f756ee0837f21d7",
      "0ff0198e313f425b8f5b394d6c346b10",
      "c5c70525b4ec4f3f90885e4303226b58",
      "69304e7f4a504a7c997fd23e3813e84a",
      "dac97c8554e64d7bbdcc542e1d6c26c1",
      "dd14256d0e6c4dc09800cddbd9c751a6",
      "8c4c3eafb1cf4019b26497de4b3d8641",
      "8894a7349e624be7b350d75d75a47bf6",
      "acec4ebd72824ee0b42819e4b94aa76d",
      "8b76ad9bcba9465f955425ffbf1242c5",
      "3054976c9fe24c46883eba2a9c4228db",
      "832ba96d80ed420eb7d12db7069c5dbf",
      "d39ada462b034ffda4b4906614329420",
      "d4edf713aabf4194ba405a6f3677fc74",
      "67ed3594fa0e4f29bccdc8c7be8348f3",
      "89c0b54e46ea4c43a9689a044deaea36",
      "b787acd3d76b49aea28613fdecd8a27f",
      "bd80e0b8ee6d4983b21204491088e822",
      "eab976b32cb448a7bc41bbf465d80714",
      "a25fdc54ae684b26839f5a440f49f89b",
      "59e7181096764d778d6e0f2606a01c24",
      "38bbad35c74d45cc9c3ad3b47735b821",
      "5df60c9e3396472088a3daf135b237bf",
      "d0a9ac3025284e7ca2b66b5cc8b6f846",
      "288e6301b78242cd8ba0fb5d83dc8639",
      "071d7c7cc1a04c2985bb8366140e7ede",
      "3e10e15b190644d78d515108a17b8874",
      "8a966c00ddbe4bcbbf9d1c9c268f9f71",
      "f359d68ae9674cef8f3333371c0c7452",
      "6de8a625f3ca43618a575271f94caf19",
      "9a5436155e1c457fb57d31e99c407c19",
      "78c786d637744ced922c470c6cf6af36",
      "6c1532c0598a47998eaae9536a94a082",
      "6bbbbe982f634359b0873103dbee98e6",
      "0f7f23e0a91a458a805bdae1da4accaa",
      "78556f4de2624fbb98880ca21801757f",
      "23700ff72128457f8144cdfaa1bb4f8e",
      "99247bcdd960444c8a586bf5627aaa9d",
      "d75a29b373504e83b55635216324c528",
      "e7ad69db77a24094a19b61cd55d6b4a8",
      "a72ead31ebbe4fbbad5fba27493cf2e8",
      "d402a537792a47f8a3fefee769015afe",
      "3c19503666354fbdb760b3f88dcc3841",
      "699c34ca09a641f9858a84a7b5fedeed",
      "68d5c43e296a400d92ccb4e3e70baefc",
      "71a7574a043642a280dd4bd024c96f5e",
      "4c40a6b3882a47df91d8a5bf3408ce74",
      "763c3caaea104c44b106496608f4dc6d",
      "4e1c101d3ef34fdcaf3390803fa07eae",
      "110b9b9a70784631841a5fa18cee9a54",
      "5ec428cfb5ae4da682f8350f56529fe7",
      "cb94ab0d84b240259630ab0f03fea31c"
     ]
    },
    "id": "H8kbABatxBhG",
    "outputId": "8b158224-7da8-4fa6-81ce-40f7337c5fd8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcf5a045b92478dbfd16a0470f89012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/781 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd17cea451ea43eca0b8eb1a02a3ad95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38eafeb773a4ba182ec6157e911d82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acec4ebd72824ee0b42819e4b94aa76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25fdc54ae684b26839f5a440f49f89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5436155e1c457fb57d31e99c407c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d402a537792a47f8a3fefee769015afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/863 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the fine-tuned `PEFT` model & tokenizer from `HFHub` and integrating the adapters with `base` model\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, \"abhxaxhbshxahxn/lora-ner-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"abhxaxhbshxahxn/lora-ner-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOwyWVAnLLwn"
   },
   "source": [
    "## Reading the data..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "referenced_widgets": [
      "cf84a945650743b78e815b7d375ba8b9",
      "4572979e22d648158713376a2f12939d",
      "3b9e09a2535f40678cca882590f45834",
      "f6e8f1aa237e4a8b8bd5d31d196bbc3b",
      "3a9c6466f3184be4af3041809e207f79",
      "94d58d3743834289ade252c9fd2da91b",
      "7bba4f3bce904b54bd91ebc26e9a91d3",
      "5080a3909a4a4e79ab614a5708a2257a",
      "f7b5ffaf41d4475ab06a5fcf7ea9e1d0",
      "b4d00c678c094f2886cfe6982415e90a",
      "e173222c116a48d59943f8e74b366afa"
     ]
    },
    "id": "4XdxjOLbyQuR",
    "outputId": "0cff767e-f118-45bd-8308-6b739498fa43"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf84a945650743b78e815b7d375ba8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset features: {'event_text': Value('string'), 'output': {'action': Value('string'), 'date': Value('string'), 'time': Value('string'), 'attendees': List(Value('string')), 'location': Value('string'), 'duration': Value('string'), 'recurrence': Value('string'), 'notes': Value('string')}}\n",
      "Number of examples: 792\n",
      "\n",
      "First 3 examples:\n",
      "{'event_text': 'Late night study session at the café on 15th, Dec 2024 at 9:00 pm for 2 hours.', 'output': {'action': 'study session', 'date': '15/12/2024', 'time': '9:00 PM', 'attendees': None, 'location': 'café', 'duration': '2 hours', 'recurrence': None, 'notes': None}}\n",
      "{'event_text': 'Hang out at the beach on 18th, Jul 2024 around 10:00 am for 3 hours or so.', 'output': {'action': 'Hang out', 'date': '18/07/2024', 'time': '10:00 AM', 'attendees': None, 'location': 'beach', 'duration': '3 hours', 'recurrence': None, 'notes': None}}\n",
      "{'event_text': 'Business lunch at that seafood spot on 2nd, Nov 2024 at 1:00 pm for roughly 2 hours.', 'output': {'action': 'Business lunch', 'date': '02/11/2024', 'time': '1:00 PM', 'attendees': None, 'location': 'that seafood spot', 'duration': '2 hours', 'recurrence': None, 'notes': None}}\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"/content/drive/MyDrive/Fine-Tuning Assignment/event_text_mapping.jsonl\"\n",
    "ds = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "\n",
    "print(\"Dataset features:\", ds.features)\n",
    "print(\"Number of examples:\", len(ds))\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i in range(min(3, len(ds))):\n",
    "    print(ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKYG3YafLVsQ"
   },
   "source": [
    "## Generating the inference for a given `example`..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWG8ajvxy1c7",
    "outputId": "2b9420eb-8696-4227-9c4f-391c183e627c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Lunch with Claire at Sunny Diner on April 29, 2024, 1:00 pm for 1 hour.\n",
      "\n",
      "\n",
      "Predicted: {'action': 'Lunch', 'date': '29/04/2024', 'time': '1:00 PM', 'attendees': ['Claire'], 'location': 'Sunny Diner', 'duration': '1 hour', 'recurrence': None, 'notes': None}\n",
      "\n",
      "\n",
      "Actual: {'action': 'Lunch', 'date': '29/04/2024', 'time': '1:00 PM', 'attendees': ['Claire'], 'location': 'Sunny Diner', 'duration': '1 hour', 'recurrence': None, 'notes': None}\n"
     ]
    }
   ],
   "source": [
    "example = ds[786]\n",
    "predicted_dict = run_generation(peft_model, tokenizer, example)\n",
    "\n",
    "print(\"Input:\", example['event_text'])\n",
    "print(\"\\n\")\n",
    "print(\"Predicted:\", predicted_dict)\n",
    "print(\"\\n\")\n",
    "print(\"Actual:\", example[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Avy37zogBt2P",
    "outputId": "f5db560e-1cb9-4a3e-cd04-350f3917f50e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {\"Gotcha! Let's discuss sports weekly on each Friday at 6 in the evening, over a ZOom call beginning 1st May,2024!\"}\n",
      "\n",
      "\n",
      "Predicted: {'action': 'Discussing sports weekly', 'date': '05/06/2024', 'time': '6:00 PM', 'attendees': None, 'location': 'Zoom', 'duration': '1 hour', 'recurrence': None, 'notes': None}\n",
      "\n",
      "\n",
      "Actual: {}\n"
     ]
    }
   ],
   "source": [
    "example = {}\n",
    "example['event_text'] = {\"Gotcha! Let's discuss sports weekly on each Friday at 6 in the evening, over a ZOom call beginning 1st May,2024!\"}\n",
    "example['output']={}\n",
    "predicted_dict = run_generation(peft_model, tokenizer, example)\n",
    "\n",
    "print(\"Input:\", example['event_text'])\n",
    "print(\"\\n\")\n",
    "print(\"Predicted:\", predicted_dict)\n",
    "print(\"\\n\")\n",
    "print(\"Actual:\", example[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCFS9FYSNvk3"
   },
   "source": [
    "-----------"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
