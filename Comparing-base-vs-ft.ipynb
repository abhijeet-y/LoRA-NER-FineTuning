{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-27T20:37:48.473709Z",
     "iopub.status.busy": "2025-07-27T20:37:48.473033Z",
     "iopub.status.idle": "2025-07-27T20:37:48.477277Z",
     "shell.execute_reply": "2025-07-27T20:37:48.476650Z",
     "shell.execute_reply.started": "2025-07-27T20:37:48.473681Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets accelerate peft\n",
    "# !pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:37:48.478930Z",
     "iopub.status.busy": "2025-07-27T20:37:48.478705Z",
     "iopub.status.idle": "2025-07-27T20:37:57.092900Z",
     "shell.execute_reply": "2025-07-27T20:37:57.092297Z",
     "shell.execute_reply.started": "2025-07-27T20:37:48.478914Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 20:37:54.179824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753648674.202539     122 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753648674.209408     122 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import torch\n",
    "import random\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from datasets import Dataset,load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:37:57.094512Z",
     "iopub.status.busy": "2025-07-27T20:37:57.094027Z",
     "iopub.status.idle": "2025-07-27T20:37:57.098172Z",
     "shell.execute_reply": "2025-07-27T20:37:57.097650Z",
     "shell.execute_reply.started": "2025-07-27T20:37:57.094491Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:37:57.099104Z",
     "iopub.status.busy": "2025-07-27T20:37:57.098861Z",
     "iopub.status.idle": "2025-07-27T20:37:57.262276Z",
     "shell.execute_reply": "2025-07-27T20:37:57.261734Z",
     "shell.execute_reply.started": "2025-07-27T20:37:57.099081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset features: {'event_text': Value(dtype='string', id=None), 'output': {'action': Value(dtype='string', id=None), 'date': Value(dtype='string', id=None), 'time': Value(dtype='string', id=None), 'attendees': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'location': Value(dtype='string', id=None), 'duration': Value(dtype='string', id=None), 'recurrence': Value(dtype='string', id=None), 'notes': Value(dtype='string', id=None)}}\n",
      "Number of examples: 792\n",
      "\n",
      "First 3 examples:\n",
      "{'event_text': 'Late night study session at the café on 15th, Dec 2024 at 9:00 pm for 2 hours.', 'output': {'action': 'study session', 'date': '15/12/2024', 'time': '9:00 PM', 'attendees': None, 'location': 'café', 'duration': '2 hours', 'recurrence': None, 'notes': None}}\n",
      "{'event_text': 'Hang out at the beach on 18th, Jul 2024 around 10:00 am for 3 hours or so.', 'output': {'action': 'Hang out', 'date': '18/07/2024', 'time': '10:00 AM', 'attendees': None, 'location': 'beach', 'duration': '3 hours', 'recurrence': None, 'notes': None}}\n",
      "{'event_text': 'Business lunch at that seafood spot on 2nd, Nov 2024 at 1:00 pm for roughly 2 hours.', 'output': {'action': 'Business lunch', 'date': '02/11/2024', 'time': '1:00 PM', 'attendees': None, 'location': 'that seafood spot', 'duration': '2 hours', 'recurrence': None, 'notes': None}}\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"/kaggle/input/llm-fine-tune-dataset/event_text_mapping.jsonl\"\n",
    "ds = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "\n",
    "print(\"Dataset features:\", ds.features)\n",
    "print(\"Number of examples:\", len(ds))\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i in range(min(3, len(ds))):\n",
    "    print(ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the base model..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:37:57.264240Z",
     "iopub.status.busy": "2025-07-27T20:37:57.263836Z",
     "iopub.status.idle": "2025-07-27T20:37:57.268904Z",
     "shell.execute_reply": "2025-07-27T20:37:57.268400Z",
     "shell.execute_reply.started": "2025-07-27T20:37:57.264222Z"
    }
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:37:57.269846Z",
     "iopub.status.busy": "2025-07-27T20:37:57.269605Z",
     "iopub.status.idle": "2025-07-27T20:37:57.284126Z",
     "shell.execute_reply": "2025-07-27T20:37:57.283526Z",
     "shell.execute_reply.started": "2025-07-27T20:37:57.269819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting up `device`\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:37:57.285055Z",
     "iopub.status.busy": "2025-07-27T20:37:57.284815Z",
     "iopub.status.idle": "2025-07-27T20:38:06.671973Z",
     "shell.execute_reply": "2025-07-27T20:38:06.671145Z",
     "shell.execute_reply.started": "2025-07-27T20:37:57.285009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba83dd1da044472f91bae10d482d5b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9603f9dd21f548dfa4278ad77b2ab5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer...!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e955a600f4e4639a9294f823cc8418d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2592fb2045fb46bbaeb7c0ef8404792d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268b77337cf04379a79f751c0b90573e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e409728858449dbf68e4a146262278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415d4bbbee2a4829bd2580c9b144d98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading base model..\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM-360M\"\n",
    "print(\"Loading Model...!\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "print(\"Loading Tokenizer...!\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:38:06.673079Z",
     "iopub.status.busy": "2025-07-27T20:38:06.672818Z",
     "iopub.status.idle": "2025-07-27T20:38:09.773536Z",
     "shell.execute_reply": "2025-07-27T20:38:09.772919Z",
     "shell.execute_reply.started": "2025-07-27T20:38:06.673049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...!\n",
      "Loading Tokenizer...!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba38d8d6bcf4c608d56aa24a90ce18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/781 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e3192d9b3645a89efaf215bc0b00d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5fa30d0e6e4729a93603e0fcaf1237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d818aba565346ff8f1f5718e612edb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42e88ecac3d4e11b8fde524484d50f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9e241cd0aa49c99ebb17c2d7f5a5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8e7d0f11be4d8db21146ec31383de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/863 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a fresh base model again to attach LoRA\n",
    "print(\"Loading Model...!\")\n",
    "model_for_lora = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "print(\"Loading Tokenizer...!\")\n",
    "lora_model = PeftModel.from_pretrained(model_for_lora, \"abhxaxhbshxahxn/lora-ner-model\")\n",
    "lora_tokenizer = AutoTokenizer.from_pretrained(\"abhxaxhbshxahxn/lora-ner-model\")\n",
    "lora_tokenizer.pad_token = lora_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating inference from the base model..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:38:09.774365Z",
     "iopub.status.busy": "2025-07-27T20:38:09.774162Z",
     "iopub.status.idle": "2025-07-27T20:38:09.778521Z",
     "shell.execute_reply": "2025-07-27T20:38:09.777773Z",
     "shell.execute_reply.started": "2025-07-27T20:38:09.774332Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_prompt(event_text):\n",
    "  \n",
    "  prompt = \"\"\"See the given example to extract the entities based on given input in JSON format.\n",
    "Example Input: Late night study session at the café on 15th, Dec 2024 at 9:00 pm for 2 hours.\n",
    "Example Output: {{'action': 'study session', 'date': '15/12/2024', 'time': '9:00 PM', 'attendees': None, 'location': 'café', 'duration': '2 hours', 'recurrence': None, 'notes': None}}\n",
    "--------------------------\n",
    "Please extract the entities for the below user input in JSON format. And do not output anything else.\n",
    "Human Input: {event_text}\n",
    "AI:\"\"\"\n",
    "\n",
    "  prompt = prompt.format(event_text=event_text)\n",
    "  \n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:38:09.779636Z",
     "iopub.status.busy": "2025-07-27T20:38:09.779326Z",
     "iopub.status.idle": "2025-07-27T20:38:17.005467Z",
     "shell.execute_reply": "2025-07-27T20:38:17.004470Z",
     "shell.execute_reply.started": "2025-07-27T20:38:09.779613Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_inference(input_text,model,tokenizer):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  input_prompt = prepare_prompt(input_text)\n",
    "\n",
    "  inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "  \n",
    "  decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  return decoded_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:38:17.008421Z",
     "iopub.status.busy": "2025-07-27T20:38:17.008127Z",
     "iopub.status.idle": "2025-07-27T20:38:24.544758Z",
     "shell.execute_reply": "2025-07-27T20:38:24.544039Z",
     "shell.execute_reply.started": "2025-07-27T20:38:17.008398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT Training session 15 - Nov - 2023 3:30 pm video\n",
      "\n",
      "\n",
      "INFERENCE \n",
      "1. What is the name of the user?\n",
      "2. What is the name of the session?\n",
      "3. What is the date of the session?\n",
      "4. What is the time of the session?\n",
      "5. What is the duration of the session?\n",
      "6. What is the recurrence of the session?\n",
      "7. What is the notes of the session?\n",
      "8. What is the name of the location of the session?\n",
      "9. What is the name of the\n",
      "\n",
      "\n",
      "ACTUAL OUTPUT {'action': 'Training session', 'date': '15/11/2023', 'time': '3:30 PM', 'attendees': None, 'location': None, 'duration': None, 'recurrence': None, 'notes': 'video'}\n"
     ]
    }
   ],
   "source": [
    "example = ds[110]\n",
    "inference_ = generate_inference(example['event_text'],base_model,base_tokenizer)\n",
    "print(\"PROMPT\",example['event_text'])\n",
    "print(\"\\n\")\n",
    "print(\"INFERENCE\",inference_)\n",
    "print(\"\\n\")\n",
    "print(\"ACTUAL OUTPUT\",example['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating inference from the fine-tuned model..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:38:24.545788Z",
     "iopub.status.busy": "2025-07-27T20:38:24.545550Z",
     "iopub.status.idle": "2025-07-27T20:38:29.704400Z",
     "shell.execute_reply": "2025-07-27T20:38:29.703618Z",
     "shell.execute_reply.started": "2025-07-27T20:38:24.545759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT Training session 15 - Nov - 2023 3:30 pm video\n",
      "\n",
      "\n",
      "INFERENCE  {'action': 'Training session', 'date': '15/11/2023', 'time': '3:30 PM', 'attendees': None, 'location': None, 'duration': None, 'recurrence': None, 'notes': None}\n",
      "\n",
      "\n",
      "ACTUAL OUTPUT {'action': 'Training session', 'date': '15/11/2023', 'time': '3:30 PM', 'attendees': None, 'location': None, 'duration': None, 'recurrence': None, 'notes': 'video'}\n"
     ]
    }
   ],
   "source": [
    "example = ds[110]\n",
    "inference_ = generate_inference(example['event_text'],lora_model,lora_tokenizer)\n",
    "print(\"PROMPT\",example['event_text'])\n",
    "print(\"\\n\")\n",
    "print(\"INFERENCE\",inference_)\n",
    "print(\"\\n\")\n",
    "print(\"ACTUAL OUTPUT\",example['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function that collectively outputs the `inference` from `base` and `fine-tuned` model...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:38:29.705455Z",
     "iopub.status.busy": "2025-07-27T20:38:29.705205Z",
     "iopub.status.idle": "2025-07-27T20:38:29.709623Z",
     "shell.execute_reply": "2025-07-27T20:38:29.709055Z",
     "shell.execute_reply.started": "2025-07-27T20:38:29.705427Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_inference(example,base_model,base_tokenizer,tuned_model,tuned_tokenizer):\n",
    "\n",
    "  input_text = example['event_text']\n",
    "  ground_truth = example['output']\n",
    "\n",
    "  ans = {}\n",
    "\n",
    "  ans['input_text'] = input_text\n",
    "    \n",
    "  ans[\"base_inference\"] = generate_inference(input_text,base_model,base_tokenizer)\n",
    "\n",
    "  ans[\"finetuned_inference\"] = generate_inference(input_text,tuned_model,tuned_tokenizer)\n",
    "  \n",
    "  ans['actual_output'] = ground_truth\n",
    "\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:38:29.710510Z",
     "iopub.status.busy": "2025-07-27T20:38:29.710243Z",
     "iopub.status.idle": "2025-07-27T20:38:42.034305Z",
     "shell.execute_reply": "2025-07-27T20:38:42.033546Z",
     "shell.execute_reply.started": "2025-07-27T20:38:29.710485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT TEXT\n",
      "Onboarding 2023-12-14 8:45 am w/ Ava & Ethan BlueJeans 30m\n",
      "INPUT_TEXT\n",
      "Onboarding 2023-12-14 8:45 am w/ Ava & Ethan BlueJeans 30m\n",
      "\n",
      "\n",
      "BASE_INFERENCE\n",
      "\n",
      "1. Human Input: Onboarding 2023-12-14 8:45 am w/ Ava & Ethan BlueJeans 30m\n",
      "2. AI:\n",
      "1. Human Input: Onboarding 2023-12-14 8:45 am w/ Ava & Ethan BlueJeans 30m\n",
      "2. AI:\n",
      "1. Human Input: Onboarding 2\n",
      "\n",
      "\n",
      "FINETUNED_INFERENCE\n",
      " {'action': 'Onboarding', 'date': '2023-12-14', 'time': '8:45 AM', 'attendees': ['Ava', 'Ethan'], 'location': 'BlueJeans', 'duration': '30m', 'recurrence': None, 'notes': None}\n",
      "\n",
      "\n",
      "ACTUAL_OUTPUT\n",
      "{'action': 'Onboarding', 'date': '2023-12-14', 'time': '8:45 AM', 'attendees': ['Ava', 'Ethan'], 'location': 'BlueJeans', 'duration': '30m', 'recurrence': None, 'notes': None}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"EVENT TEXT\")\n",
    "print(ds[300]['event_text'])\n",
    "\n",
    "params={}\n",
    "params['example'] = ds[300]\n",
    "params['base_model']= base_model\n",
    "params['base_tokenizer']= base_tokenizer\n",
    "params['tuned_model']= lora_model\n",
    "params['tuned_tokenizer']= lora_tokenizer\n",
    "\n",
    "ans = compare_inference(**params)\n",
    "for k,v in ans.items():\n",
    "  print(k.upper(),v,sep=\"\\n\")\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `inference_df` to store and compare the inference for `base` vs `fine-tuned`..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:38:42.035532Z",
     "iopub.status.busy": "2025-07-27T20:38:42.035263Z",
     "iopub.status.idle": "2025-07-27T20:58:54.619266Z",
     "shell.execute_reply": "2025-07-27T20:58:54.618540Z",
     "shell.execute_reply.started": "2025-07-27T20:38:42.035512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [20:12<00:00, 12.13s/it]\n"
     ]
    }
   ],
   "source": [
    "params={}\n",
    "params['base_model']= base_model\n",
    "params['base_tokenizer']= base_tokenizer\n",
    "params['tuned_model']= lora_model\n",
    "params['tuned_tokenizer']= lora_tokenizer\n",
    "\n",
    "result_df = {}\n",
    "for idx in tqdm(range(100),dynamic_ncols=True, leave=True):\n",
    "    unique_rand_ints = random.sample(range(0, 791), 1)\n",
    "    params['example'] = ds[unique_rand_ints]\n",
    "    ans = compare_inference(**params)\n",
    "\n",
    "    for k,v in ans.items():\n",
    "        if k in result_df.keys():\n",
    "            result_df[k].append(v)\n",
    "        else:\n",
    "            result_df[k]=[]\n",
    "            result_df[k].append(v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:58:54.620571Z",
     "iopub.status.busy": "2025-07-27T20:58:54.620231Z",
     "iopub.status.idle": "2025-07-27T20:58:54.678911Z",
     "shell.execute_reply": "2025-07-27T20:58:54.678184Z",
     "shell.execute_reply.started": "2025-07-27T20:58:54.620542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>base_inference</th>\n",
       "      <th>finetuned_inference</th>\n",
       "      <th>actual_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Plan a workshop with Olivia, Sophia, and Luca...</td>\n",
       "      <td>{'action': 'plan a workshop with Olivia, Soph...</td>\n",
       "      <td>{'action': 'Plan a workshop', 'date': '18/05/...</td>\n",
       "      <td>[{'action': 'Plan a workshop', 'date': '18/05/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Roadmap sync 27th, Dec 2025 12:00 pm w/ Joe Z...</td>\n",
       "      <td>['Roadmap sync 27th, Dec 2025 12:00 pm w/ Joe...</td>\n",
       "      <td>{'action': 'Roadmap sync', 'date': '27/12/202...</td>\n",
       "      <td>[{'action': 'Roadmap sync', 'date': '27/12/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Discuss proposal 2023-12-07 11:15 am w/ Emma ...</td>\n",
       "      <td>['Discuss proposal 2023-12-07 11:15\\u202fEmma...</td>\n",
       "      <td>{'action': 'Discuss proposal 2023-12-07', 'da...</td>\n",
       "      <td>[{'action': 'Discuss proposal', 'date': '2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Hiring panel 2024-03-03 1:30 pm Sophie, Will,...</td>\n",
       "      <td>['Hiring panel 2024-03-03 1:30pm Sophie, Will...</td>\n",
       "      <td>{'action': 'Hiring panel', 'date': '2024-03-0...</td>\n",
       "      <td>[{'action': 'Hiring panel', 'date': '2024-03-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Organize time for a roadmap discussion on May...</td>\n",
       "      <td>['Organize time for a roadmap discussion on M...</td>\n",
       "      <td>{'action': 'Organize time for a roadmap discu...</td>\n",
       "      <td>[{'action': 'roadmap discussion', 'date': '21/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  [Plan a workshop with Olivia, Sophia, and Luca...   \n",
       "1  [Roadmap sync 27th, Dec 2025 12:00 pm w/ Joe Z...   \n",
       "2  [Discuss proposal 2023-12-07 11:15 am w/ Emma ...   \n",
       "3  [Hiring panel 2024-03-03 1:30 pm Sophie, Will,...   \n",
       "4  [Organize time for a roadmap discussion on May...   \n",
       "\n",
       "                                      base_inference  \\\n",
       "0   {'action': 'plan a workshop with Olivia, Soph...   \n",
       "1   ['Roadmap sync 27th, Dec 2025 12:00 pm w/ Joe...   \n",
       "2   ['Discuss proposal 2023-12-07 11:15\\u202fEmma...   \n",
       "3   ['Hiring panel 2024-03-03 1:30pm Sophie, Will...   \n",
       "4   ['Organize time for a roadmap discussion on M...   \n",
       "\n",
       "                                 finetuned_inference  \\\n",
       "0   {'action': 'Plan a workshop', 'date': '18/05/...   \n",
       "1   {'action': 'Roadmap sync', 'date': '27/12/202...   \n",
       "2   {'action': 'Discuss proposal 2023-12-07', 'da...   \n",
       "3   {'action': 'Hiring panel', 'date': '2024-03-0...   \n",
       "4   {'action': 'Organize time for a roadmap discu...   \n",
       "\n",
       "                                       actual_output  \n",
       "0  [{'action': 'Plan a workshop', 'date': '18/05/...  \n",
       "1  [{'action': 'Roadmap sync', 'date': '27/12/202...  \n",
       "2  [{'action': 'Discuss proposal', 'date': '2023-...  \n",
       "3  [{'action': 'Hiring panel', 'date': '2024-03-0...  \n",
       "4  [{'action': 'roadmap discussion', 'date': '21/...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result_df)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:59:06.462455Z",
     "iopub.status.busy": "2025-07-27T20:59:06.461709Z",
     "iopub.status.idle": "2025-07-27T20:59:06.473465Z",
     "shell.execute_reply": "2025-07-27T20:59:06.472813Z",
     "shell.execute_reply.started": "2025-07-27T20:59:06.462426Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df.to_csv('Base_vs_FT_inference.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking performance for the NER task..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:59:08.800803Z",
     "iopub.status.busy": "2025-07-27T20:59:08.799882Z",
     "iopub.status.idle": "2025-07-27T20:59:08.806705Z",
     "shell.execute_reply": "2025-07-27T20:59:08.806108Z",
     "shell.execute_reply.started": "2025-07-27T20:59:08.800774Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_dicts(result_dict, output_dict):\n",
    "    comparison = {}\n",
    "    for key in output_dict.keys():\n",
    "        expected = output_dict[key]\n",
    "        predicted = result_dict.get(key, None)\n",
    "\n",
    "        # Normalize string values\n",
    "        if isinstance(expected, str) and isinstance(predicted, str):\n",
    "            expected = expected.strip().lower()\n",
    "            predicted = predicted.strip().lower()\n",
    "\n",
    "        # For lists: compare ignoring order\n",
    "        if isinstance(expected, list) and isinstance(predicted, list):\n",
    "            correct = sorted(expected) == sorted(predicted)\n",
    "        else:\n",
    "            correct = expected == predicted\n",
    "\n",
    "        comparison[key] = {\n",
    "            \"expected\": output_dict[key],\n",
    "            \"predicted\": result_dict.get(key, None),\n",
    "            \"match\": correct\n",
    "        }\n",
    "\n",
    "    return comparison\n",
    "\n",
    "def dict_accuracy(result_dict, output_dict):\n",
    "    comp = compare_dicts(result_dict, output_dict)\n",
    "    matches = sum(1 for k in comp if comp[k]['match'])\n",
    "    acc = {key: int(comp[key][\"match\"]) for key in comp}\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:59:10.033502Z",
     "iopub.status.busy": "2025-07-27T20:59:10.032914Z",
     "iopub.status.idle": "2025-07-27T20:59:10.038988Z",
     "shell.execute_reply": "2025-07-27T20:59:10.038247Z",
     "shell.execute_reply.started": "2025-07-27T20:59:10.033477Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(result_df,column_name):\n",
    "    base_accuracy={}\n",
    "    base_accuracy_df = {}\n",
    "    comp = ds[0]['output'].keys()\n",
    "    \n",
    "    for idx in range(result_df.shape[0]):\n",
    "        # print(idx)\n",
    "        \n",
    "        try:\n",
    "            base_output = ast.literal_eval(result_df.loc[idx,column_name])\n",
    "            matches = dict_accuracy(base_output,result_df.loc[idx,'actual_output'][0])\n",
    "    \n",
    "        \n",
    "        except:\n",
    "            base_output = {}\n",
    "            matches = {key: 0 for key in comp}\n",
    "    \n",
    "        \n",
    "        for com in comp:\n",
    "            if com in base_accuracy_df.keys():\n",
    "                base_accuracy_df[com].append(matches[com])\n",
    "            else:\n",
    "                base_accuracy_df[com] = []\n",
    "                base_accuracy_df[com].append(matches[com])\n",
    "\n",
    "    return pd.DataFrame(base_accuracy_df).describe().loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:59:10.187638Z",
     "iopub.status.busy": "2025-07-27T20:59:10.186892Z",
     "iopub.status.idle": "2025-07-27T20:59:10.248033Z",
     "shell.execute_reply": "2025-07-27T20:59:10.247466Z",
     "shell.execute_reply.started": "2025-07-27T20:59:10.187613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE ACCURACY IN % FOR THE `BASE` MODEL OVER `100` RANDOM SAMPLES...!\n",
      "\n",
      "\n",
      "ACTION              0.0\n",
      "DATE                0.0\n",
      "TIME                0.0\n",
      "ATTENDEES           0.0\n",
      "LOCATION            0.0\n",
      "DURATION            0.0\n",
      "RECURRENCE          0.0\n",
      "NOTES               0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Accuracy in % for the `Base` model over `100` random samples...!\".upper())\n",
    "print('\\n')\n",
    "for k,v in dict(calculate_accuracy(result_df,column_name='base_inference')).items():\n",
    "    print(k.upper(),v*100,sep=' '*(20-len(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T20:59:10.353668Z",
     "iopub.status.busy": "2025-07-27T20:59:10.353489Z",
     "iopub.status.idle": "2025-07-27T20:59:10.377230Z",
     "shell.execute_reply": "2025-07-27T20:59:10.376411Z",
     "shell.execute_reply.started": "2025-07-27T20:59:10.353654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE ACCURACY IN % FOR THE `BASE` MODEL OVER `100` RANDOM SAMPLES...!\n",
      "\n",
      "\n",
      "ACTION              83.0\n",
      "DATE                85.0\n",
      "TIME                94.0\n",
      "ATTENDEES           82.0\n",
      "LOCATION            90.0\n",
      "DURATION            89.0\n",
      "RECURRENCE          93.0\n",
      "NOTES               97.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Accuracy in % for the `Base` model over `100` random samples...!\".upper())\n",
    "print('\\n')\n",
    "for k,v in dict(calculate_accuracy(result_df,column_name='finetuned_inference')).items():\n",
    "    print(k.upper(),v*100,sep=' '*(20-len(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7941971,
     "sourceId": 12575520,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
